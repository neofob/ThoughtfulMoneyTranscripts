# Settings for AI model
* AI model, Mixtral 8x22B: https://huggingface.co/MaziyarPanahi/Mixtral-8x22B-v0.1-GGUF
* Quantization: 6bit
* Context length: 40960

**Note:**
* Quantization can be used at higher value such as 8bit if you have enough VRAM
* Max context length for Mixtral 8x22B is 65536. Use this if you have enough VRAM
